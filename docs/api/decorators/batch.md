# The current object

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->

fdo


<DocSection type="decorator" name="batch" module="metaflow" show_import="False" heading_level="3" link="https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/aws/batch/batch_decorator.py#L30">
<SigArgSection>
<SigArg name="..." />
</SigArgSection>
<Description summary="Step decorator to specify that this step should execute on AWS Batch." extended_summary="This decorator indicates that your step should execute on AWS Batch. Note\nthat you can apply this decorator automatically to all steps using the\n```--with batch``` argument when calling run/resume. Step level decorators\nwithin the code are overrides and will force a step to execute on AWS Batch\nregardless of the ```--with``` specification.\n\nTo use, annotate your step as follows:\n```\n@batch\n@step\ndef my_step(self):\n    ...\n```\nParameters\n----------\ncpu : int\n    Number of CPUs required for this step. Defaults to 1. If @resources is\n    also present, the maximum value from all decorators is used\ngpu : int\n    Number of GPUs required for this step. Defaults to 0. If @resources is\n    also present, the maximum value from all decorators is used\nmemory : int\n    Memory size (in MB) required for this step. Defaults to 4096. If\n    @resources is also present, the maximum value from all decorators is\n    used\nimage : string\n    Docker image to use when launching on AWS Batch. If not specified, a\n    default docker image mapping to the current version of Python is used\nqueue : string\n    AWS Batch Job Queue to submit the job to. Defaults to the one\n    specified by the environment variable METAFLOW_BATCH_JOB_QUEUE\niam_role : string\n    AWS IAM role that AWS Batch container uses to access AWS cloud resources\n    (Amazon S3, Amazon DynamoDb, etc). Defaults to the one specified by the\n    environment variable METAFLOW_ECS_S3_ACCESS_IAM_ROLE\nexecution_role : string\n    AWS IAM role that AWS Batch can use to trigger AWS Fargate tasks.\n    Defaults to the one determined by the environment variable\n    METAFLOW_ECS_FARGATE_EXECUTION_ROLE https://docs.aws.amazon.com/batch/latest/userguide/execution-IAM-role.html\nshared_memory : int\n    The value for the size (in MiB) of the /dev/shm volume for this step.\n    This parameter maps to the --shm-size option to docker run.\nmax_swap : int\n    The total amount of swap memory (in MiB) a container can use for this\n    step. This parameter is translated to the --memory-swap option to\n    docker run where the value is the sum of the container memory plus the\n    max_swap value.\nswappiness : int\n    This allows you to tune memory swappiness behavior for this step.\n    A swappiness value of 0 causes swapping not to happen unless absolutely\n    necessary. A swappiness value of 100 causes pages to be swapped very\n    aggressively. Accepted values are whole numbers between 0 and 100." />
<ParamSection name="Attributes">
	<Parameter name="package_sha" />
	<Parameter name="package_url" />
	<Parameter name="run_time_limit" />
</ParamSection>
</DocSection>

